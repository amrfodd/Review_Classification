{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a7da9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re \n",
    "import nltk\n",
    "import string\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6879fd2d",
   "metadata": {},
   "source": [
    "### 1) Load data and concat them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "baf560e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>crust good</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tasty texture nasty</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stopped late may bank holiday rick steve recom...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>selection menu great prices</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>getting angry want damn pho</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Sentiment  Class\n",
       "0                                         crust good      0\n",
       "1                                tasty texture nasty      0\n",
       "2  stopped late may bank holiday rick steve recom...      1\n",
       "3                        selection menu great prices      1\n",
       "4                        getting angry want damn pho      0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_table(r'../Data/cleaned_reviews.csv', delimiter = ',')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f0343be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cd937d",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "\n",
    "use the CountVectorizer to vectorize sentences. It takes the words of each sentence and creates a vocabulary of all the unique words in the sentences. This vocabulary can then be used to create a feature vector of the count of the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09d75d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine data and target\n",
    "X = df['Sentiment']\n",
    "y = df.iloc[:, -1].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c788a991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training, validation and Test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec8f5c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (2193,)\n",
      "Shape of X_test: (549,)\n",
      "Shape of train_vectors: (2193, 4584)\n",
      "Shape of test_vectors: (549, 4584)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "# I will use TF-IDF method to extract the text features.\n",
    "\n",
    "tf_vec = TfidfVectorizer(tokenizer=None, stop_words=None, max_df=0.75, max_features=2000, lowercase=False,\n",
    "                         ngram_range=(1,2), use_idf=False, sublinear_tf=True, min_df=5, norm='l2',\n",
    "                         encoding='latin-1')\n",
    "'''\n",
    "\n",
    "## Vectorization of data\n",
    "## Vectorize the data using Bag of words (BOW)\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
    "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizer.tokenize, stop_words=stop_words)\n",
    "\n",
    "\n",
    "train_features = vectorizer.fit(X_train)\n",
    "train_features = vectorizer.transform(X_train)\n",
    "test_features = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "print('Shape of X_train:',X_train.shape)\n",
    "print('Shape of X_test:',X_test.shape)\n",
    "print('Shape of train_vectors:',train_features.shape)\n",
    "print('Shape of test_vectors:',test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc842ebc",
   "metadata": {},
   "source": [
    "This vocabulary serves also as an index of each word. Now, you can take each sentence and get the word occurrences of the words based on the previous vocabulary. The vocabulary consists of all five words in our sentences, each representing one word in the vocabulary. When you take the previous two sentences and transform them with the CountVectorizer you will get a vector representing the count of each word of the sentence:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3e74f6",
   "metadata": {},
   "source": [
    "#### Save Vectors"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d721528",
   "metadata": {},
   "source": [
    "#  save tfidf vectorizer\n",
    "with open('../vectors/train_vector.pkl', 'wb') as handle:\n",
    "    pickle.dump(train_features, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../vectors/val_vector.pkl', 'wb') as handle:\n",
    "    pickle.dump(val_features, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('../vectors/test_vector.pkl', 'wb') as handle:\n",
    "    pickle.dump(test_features, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b69a4817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  save BagofWords vectorizer\n",
    "with open('../vectors/train_vector.pkl', 'wb') as handle:\n",
    "    pickle.dump(train_features, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../vectors/test_vector.pkl', 'wb') as handle:\n",
    "    pickle.dump(test_features, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2891ca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  save labels vectorizer\n",
    "with open('../vectors/y_train.pkl', 'wb') as handle:\n",
    "    pickle.dump(y_train, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('../vectors/y_test.pkl', 'wb') as handle:\n",
    "    pickle.dump(y_test, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
